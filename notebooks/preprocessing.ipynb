{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Converting datasets from BRAT-standoff to jsonlines format"
   ],
   "metadata": {
    "id": "dfx0vdZCgFT5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -q jsonlines\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "from spacy import displacy\n",
    "from spacy.tokens import *\n",
    "import random\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "from pathlib import Path\n",
    "import jsonlines\n",
    "from tqdm.notebook import tqdm"
   ],
   "metadata": {
    "id": "RrzN74LjI7nr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def get_all_labels(directory, count=False):\n",
    "  nlp = spacy.blank('ru')\n",
    "  config = {\"punct_chars\": ['\\n\\n']}\n",
    "  nlp.add_pipe(\"sentencizer\", config=config)\n",
    "  labels = []\n",
    "  for filename in os.listdir(directory):\n",
    "      f = os.path.join(directory, filename)\n",
    "      if os.path.isfile(f) and filename.endswith('.txt'):\n",
    "          text = get_text(f)\n",
    "          ann = get_ann(f[:-3] + 'ann')\n",
    "          doc = nlp(text)\n",
    "          for entry in ann:\n",
    "              if len(entry) == 5:\n",
    "                label, span1_start, span1_end, span2_start, span2_end = entry\n",
    "              elif len(entry) == 7:\n",
    "                label, span1_start, span1_end, span2_start, span2_end, span3_start, span3_end = entry\n",
    "              else:\n",
    "                label, span_start, span_end = entry\n",
    "              labels.append(label)\n",
    "  if count is True:\n",
    "    counter = dict.fromkeys(labels)\n",
    "    values, counts = np.unique(labels, return_counts=True)\n",
    "    for pair in zip(values, counts):\n",
    "      counter[pair[0]] = pair[1]\n",
    "    return counter\n",
    "  else:\n",
    "    return set(labels)\n",
    "  \n",
    "\n",
    "def get_colors(directory):\n",
    "  if directory.endswith('.txt'):\n",
    "    labels = get_all_labels(os.path.dirname(directory))\n",
    "  else:\n",
    "    labels = get_all_labels(directory)\n",
    "  colors = dict.fromkeys(labels)\n",
    "  for key in colors.keys():\n",
    "    rand = lambda: random.randint(100, 220)\n",
    "    colors[key] = '#%02X%02X%02X' % (rand(), rand(), rand())\n",
    "  return colors\n",
    "\n",
    "def get_ann(path, with_token=False):\n",
    "  ann_file = open(path, 'r')\n",
    "  ann = ann_file.read().split('\\n')\n",
    "  spl = []\n",
    "  for i, line in enumerate(ann):\n",
    "    splitted = []\n",
    "    if line != '' and line[0] == 'T':\n",
    "      span = line.split('\\t')[1:]\n",
    "      entry = span[0].split(' ')\n",
    "      if len(entry) > 3:\n",
    "          for i, e in enumerate(entry):\n",
    "            if ';' in e:\n",
    "              temp = e.split(';')\n",
    "              chunk = [temp[0]] + [temp[1]]\n",
    "              splitted += chunk\n",
    "            else:\n",
    "              chunk = entry[i]\n",
    "              splitted += [chunk]\n",
    "          if with_token:\n",
    "            spl.append(splitted + [span[1]])\n",
    "          else:\n",
    "            spl.append(splitted)\n",
    "      else:\n",
    "          if with_token:\n",
    "            spl.append(entry + [span[1]])\n",
    "          else:\n",
    "            spl.append(entry)\n",
    "  return spl\n",
    "\n",
    "\n",
    "def get_doc_file(path):\n",
    "  nlp = spacy.blank('ru')\n",
    "  all_spans = []\n",
    "  filename = path.split('/')[-1]\n",
    "  if os.path.isfile(path) and filename.endswith('.txt'):\n",
    "      text = get_text(path)\n",
    "      ann = get_ann(path[:-3] + 'ann')\n",
    "      doc = nlp(text)\n",
    "      spans = []\n",
    "      for entry in ann:\n",
    "          if len(entry) == 5:\n",
    "            label, span1_start, span1_end, span2_start, span2_end = entry\n",
    "            span1 = doc.char_span(int(span1_start), int(span1_end), label=label)\n",
    "            span2 = doc.char_span(int(span2_start), int(span2_end), label=label)\n",
    "            if span1 is not None:\n",
    "              spans.append(span1)\n",
    "            if span2 is not None:\n",
    "              spans.append(span2)\n",
    "          elif len(entry) == 7:\n",
    "            label, span1_start, span1_end, span2_start, span2_end, span3_start, span3_end = entry\n",
    "            span1 = doc.char_span(int(span1_start), int(span1_end), label=label)\n",
    "            span2 = doc.char_span(int(span2_start), int(span2_end), label=label)\n",
    "            span3 = doc.char_span(int(span3_start), int(span3_end), label=label)\n",
    "            if span1 is not None:\n",
    "              spans.append(span1)\n",
    "            if span2 is not None:\n",
    "              spans.append(span2)\n",
    "            if span3 is not None:\n",
    "              spans.append(span3)\n",
    "          else:\n",
    "            label, span_start, span_end = entry\n",
    "            span = doc.char_span(int(span_start), int(span_end), label=label)\n",
    "            if span is not None:\n",
    "              spans.append(span)\n",
    "      doc.spans['custom'] = spans\n",
    "      return doc\n",
    "\n",
    "def get_doc_dir(directory):\n",
    "  docs = []\n",
    "  for filename in os.listdir(directory):\n",
    "      f = os.path.join(directory, filename)\n",
    "      if os.path.isfile(f) and filename.endswith('.txt'):\n",
    "        doc = get_doc_file(f)\n",
    "        docs.append(doc)\n",
    "  return docs"
   ],
   "metadata": {
    "id": "yRBcQrLgLpah"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def max_len(filename):\n",
    "  max_len = 0\n",
    "  with jsonlines.open(filename, mode='r') as reader: \n",
    "    for obj in reader:\n",
    "      if len(obj['tokens']) > max_len:\n",
    "        max_len = len(obj['tokens'])\n",
    "        print(len(obj['tokens']))\n",
    "        print(obj['tokens'])\n",
    "        print(obj['entity_mentions'])\n",
    "\n",
    "def get_text(path):\n",
    "  text_file = open(path, 'r', encoding='utf8')\n",
    "  text = ' '.join(text_file.read().split('\\n'))\n",
    "  text = text.replace(u'\\xa0', u' ')\n",
    "  text = text.rstrip(' ')\n",
    "  return text\n",
    "\n",
    "def get_sentences(path):\n",
    "  text_file = open(path, 'r', encoding='utf8')\n",
    "  if 'bio' in path:\n",
    "    sentences = text_file.read().split('\\n')\n",
    "  else:\n",
    "    sentences = text_file.read().split('\\n\\n')\n",
    "    \n",
    "  sentences = list(filter(lambda x: x != '', sentences))\n",
    "  for i, sent in enumerate(sentences):\n",
    "    sentences[i] = sentences[i].replace(u'\\xa0', u' ')\n",
    "    sentences[i] = sentences[i].replace(u'\\n', u' ')\n",
    "    sentences[i] = sentences[i].rstrip(' ')\n",
    "  return sentences\n",
    "\n",
    "def get_sentence_anns(path, filter):\n",
    "  tokens = spacy.blank(\"ru\") \n",
    "  filename = path.split('/')[-1]\n",
    "  sent_anns = []\n",
    "  if os.path.isfile(path) and filename.endswith('.txt'):\n",
    "    doc = get_doc_file(path)\n",
    "    sentences = get_sentences(path)\n",
    "    bound_lo = 0\n",
    "    bound_hi = 0\n",
    "    for i in range(len(sentences)):\n",
    "      bound_hi = bound_lo + len(tokens(sentences[i]))\n",
    "      sent_spans = []\n",
    "      for span in doc.spans['custom']:\n",
    "        if span.start >= bound_lo and span.end <= bound_hi:\n",
    "          span_start = int(span.start) - bound_lo\n",
    "          span_end = int(span.end) - bound_lo\n",
    "        else:\n",
    "          continue\n",
    "        if filter is not None:\n",
    "          if str(span.label_) in filter:\n",
    "              sent_spans.append([span.label_, span_start, span_end, span.text])\n",
    "          elif str(span.label_) == \"DISO\":\n",
    "              sent_spans.append([\"DISEASE\", span_start, span_end, span.text])\n",
    "          else:\n",
    "            continue\n",
    "        else:\n",
    "          sent_spans.append([span.label_, span_start, span_end, span.text])\n",
    "      sent_ann = sent_to_json([t.text for t in tokens(sentences[i])], sent_spans)\n",
    "      if sent_ann['entity_mentions'] != []:\n",
    "        sent_anns.append(sent_ann)\n",
    "      if 'bio' in path:\n",
    "        bound_lo = bound_hi\n",
    "      else:\n",
    "        bound_lo = bound_hi + 1\n",
    "  return sent_anns\n",
    "\n",
    "\n",
    "def sent_to_json(tokens, spans):\n",
    "  # building json for one 'sentence'\n",
    "  entity_mentions = []\n",
    "  for span in spans:\n",
    "    entity_mentions.append({\"entity_type\": str(span[0]), \"start\": int(span[1]), \"end\": int(span[2]), \"text\": span[3]})\n",
    "  ann_json = {\"tokens\": tokens, \"entity_mentions\": entity_mentions}\n",
    "  return ann_json\n",
    "\n",
    "def file_to_json(path, filter):\n",
    "  # building json for whole file\n",
    "  filename = path.split('/')[-1]\n",
    "  if os.path.isfile(path) and filename.endswith('.txt'):\n",
    "    doc = get_doc_file(path)\n",
    "    tokens = ([t.text for t in doc])\n",
    "    entity_mentions = []\n",
    "    for span in doc.spans['custom']:\n",
    "      if filter is not None:\n",
    "        if str(span.label_) in filter:\n",
    "          entity_mentions.append({\"entity_type\": str(span.label_), \"start\": int(span.start), \"end\":int(span.end), \"text\": span.text})\n",
    "        elif str(span.label_) == \"DISO\":\n",
    "          entity_mentions.append({\"entity_type\": \"DISEASE\", \"start\": int(span.start), \"end\":int(span.end), \"text\": span.text})\n",
    "        else:\n",
    "          continue\n",
    "      else:\n",
    "          entity_mentions.append({\"entity_type\": str(span.label_), \"start\": int(span.start), \"end\":int(span.end), \"text\": span.text})\n",
    "    ann_json = {\"tokens\": tokens, \"entity_mentions\": entity_mentions}\n",
    "    return ann_json\n",
    "\n",
    "def valid_labeling(path, filter, debug=False):\n",
    "  # Validating sentence-wise labeling by comparing with full document labeling\n",
    "  with open(path, 'r') as reader:\n",
    "    text = ''.join(reader.readlines())\n",
    "    if '\\n\\n\\n' in text or '\\n\\n\\n\\n' in text or '\\n\\n\\n\\n\\n' in text:\n",
    "      # print('corrupted file detected with name', path)\n",
    "      return False, None\n",
    "\n",
    "  sent_anns = get_sentence_anns(path, filter)\n",
    "  sum_ent = 0\n",
    "  for ann in sent_anns:\n",
    "    sum_ent += len(ann['entity_mentions'])\n",
    "\n",
    "    if debug:\n",
    "      for ent in ann['entity_mentions']:\n",
    "        print(ent)\n",
    "\n",
    "  json = file_to_json(path, filter)\n",
    "  anns = json['entity_mentions']\n",
    "\n",
    "  if debug:\n",
    "    print('################################')\n",
    "    for ann in anns:\n",
    "      print(ann)\n",
    "    print(sum_ent, len(anns))\n",
    "\n",
    "  if sum_ent == len(anns):\n",
    "    return True, sent_anns\n",
    "  else:\n",
    "    return False, None\n",
    "\n",
    "def dir_to_jsonlines(directory, filename, filter=None):\n",
    "  wrong_cnt = 0\n",
    "  with jsonlines.open(filename, mode='w') as writer:\n",
    "    for filename in tqdm(os.listdir(directory)):\n",
    "        f = os.path.join(directory, filename)\n",
    "        if os.path.isfile(f) and filename.endswith('.txt'):\n",
    "          valid, sent_anns = valid_labeling(f, filter)\n",
    "          if valid:\n",
    "            json_lines = sent_anns\n",
    "            writer.write_all(json_lines)\n",
    "          else:\n",
    "            wrong_cnt += 1\n",
    "            # print(f'file {f} was labeled wrong, please check')\n",
    "    writer.close()\n",
    "  \n",
    "  print(f'Conversion of {directory} done, {wrong_cnt} files were invalid')"
   ],
   "metadata": {
    "id": "5T6XMO0WcJrZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "nerel_train = '../nlp_project/datasets/NEREL-v1.1/train'\n",
    "nerel_test = '../nlp_project/datasets/NEREL-v1.1/test'\n",
    "nerel_dev = '../nlp_project/datasets/NEREL-v1.1/dev'\n",
    "\n",
    "nerel_bio_train = '../nlp_project/datasets/nerel-bio-v1.0/train'\n",
    "nerel_bio_test = '../nlp_project/datasets/nerel-bio-v1.0/test'\n",
    "nerel_bio_dev = '../nlp_project/datasets/nerel-bio-v1.0/dev'\n",
    "\n",
    "nerel_labels = get_all_labels(nerel_train, count=True)\n",
    "nerel_bio_labels = get_all_labels(nerel_bio_train, count=True)\n",
    "\n",
    "filtered_nerel_labels = set(filter(lambda x: nerel_labels[x] > 50, nerel_labels.keys()))\n",
    "filtered_nerel_bio_labels = set(filter(lambda x: nerel_bio_labels[x] > 50, nerel_bio_labels.keys()))"
   ],
   "metadata": {
    "id": "Zph4x4sruaBt"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "out_path_nerel = '../nlp_project/datasets/outputs/nerel'\n",
    "if not os.path.exists(out_path_nerel):\n",
    "    os.mkdir(out_path_nerel)\n",
    "    dir_to_jsonlines(nerel_train, f'{out_path_nerel}/train.jsonl', filter=filtered_nerel_labels)\n",
    "    dir_to_jsonlines(nerel_test, f'{out_path_nerel}/test.jsonl', filter=filtered_nerel_labels)\n",
    "    dir_to_jsonlines(nerel_dev, f'{out_path_nerel}/dev.jsonl', filter=filtered_nerel_labels)\n",
    "else:\n",
    "  print(f'Directory {out_path_nerel} already exists')\n",
    "\n",
    "out_path_nerel_bio = '../nlp_project/datasets/outputs/nerel_bio'\n",
    "if not os.path.exists(out_path_nerel_bio):\n",
    "    os.mkdir(out_path_nerel_bio)\n",
    "    dir_to_jsonlines(nerel_bio_train, f'{out_path_nerel_bio}/train.jsonl', filter=filtered_nerel_bio_labels)\n",
    "    dir_to_jsonlines(nerel_bio_test, f'{out_path_nerel_bio}/test.jsonl', filter=filtered_nerel_bio_labels)\n",
    "    dir_to_jsonlines(nerel_bio_dev, f'{out_path_nerel_bio}/dev.jsonl', filter=filtered_nerel_bio_labels)\n",
    "else:\n",
    "  print(f'Directory {out_path_nerel_bio} already exists')"
   ],
   "metadata": {
    "id": "GCceCptS92F2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "92c75246-85c9-4837-96a9-895e15b0bcad"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "common_labels = sorted(list(filtered_nerel_labels & filtered_nerel_bio_labels) + ['DISEASE'])\n",
    "print('Common labels btwn NEREL and NEREL-BIO:', common_labels)\n",
    "\n",
    "out_path_common_labels_nerel = '../nlp_project/datasets/outputs/nerel_common_labels'\n",
    "if not os.path.exists(out_path_common_labels_nerel):\n",
    "    os.mkdir(out_path_common_labels_nerel)\n",
    "    dir_to_jsonlines(nerel_train, f'{out_path_common_labels_nerel}/train.jsonl', filter=common_labels)\n",
    "    dir_to_jsonlines(nerel_test, f'{out_path_common_labels_nerel}/test.jsonl', filter=common_labels)\n",
    "    dir_to_jsonlines(nerel_dev, f'{out_path_common_labels_nerel}/dev.jsonl', filter=common_labels)\n",
    "else:\n",
    "    print(f'Directory {out_path_common_labels_nerel} already exists')\n",
    "\n",
    "\n",
    "out_path_common_labels_nerel_bio = '../nlp_project/datasets/outputs/nerel_bio_common_labels'\n",
    "if not os.path.exists(out_path_common_labels_nerel_bio):\n",
    "    os.mkdir(out_path_common_labels_nerel_bio)\n",
    "    dir_to_jsonlines(nerel_bio_train, f'{out_path_common_labels_nerel_bio}/train.jsonl', filter=common_labels)\n",
    "    dir_to_jsonlines(nerel_bio_test, f'{out_path_common_labels_nerel_bio}/test.jsonl', filter=common_labels)\n",
    "    dir_to_jsonlines(nerel_bio_dev, f'{out_path_common_labels_nerel_bio}/dev.jsonl', filter=common_labels)\n",
    "else:\n",
    "    print(f'Directory {out_path_common_labels_nerel_bio} already exists')"
   ],
   "metadata": {
    "id": "-Lpj-dG89z6Z",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9b15c337-2a3c-49ec-e05b-0b538d734df8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Misc"
   ],
   "metadata": {
    "id": "TkqKmVjxf_5P"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "categories_nerel = {'Category': list(filtered_nerel_labels)}\n",
    "\n",
    "df_categories_nerel = pd.DataFrame(data=categories_nerel)\n",
    "\n",
    "categories_nerel_bio = {'Category': list(filtered_nerel_bio_labels)}\n",
    "\n",
    "df_categories_nerel_bio = pd.DataFrame(data=categories_nerel_bio)\n",
    "\n",
    "\n",
    "categories_common = {'Category': common_labels}\n",
    "\n",
    "df_categories_common = pd.DataFrame(data=categories_common)"
   ],
   "metadata": {
    "id": "JWiHHy_jnLVD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_categories_nerel"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "oD_UxgzgoK8E",
    "outputId": "3fc48639-20b9-4eb8-8091-87bb0d244714"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_categories_nerel_bio"
   ],
   "metadata": {
    "id": "kT942NVXoO1H",
    "outputId": "30ea7285-3d21-48a2-c7ac-0aa813dbd583",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df_categories_common"
   ],
   "metadata": {
    "id": "j5RyxkjioQ0O",
    "outputId": "8690f89c-43b3-4989-ddcf-94487de57be1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "########################################WWWWWWWWWRRRRRRRRRRRROOOOOOOOOONNNNNNNNGGGGGGGGGG#############################################################\n",
    "# def filter_keys(dict_, lst):\n",
    "#     new_dict = {}\n",
    "#     for key in dict_:\n",
    "#         if key in lst:\n",
    "#             new_dict[key] = dict_[key]\n",
    "#     return new_dict\n",
    "\n",
    "# nerel_labels_test = get_all_labels(nerel_test, count=True)\n",
    "# nerel_bio_labels_test = get_all_labels(nerel_bio_test, count=True)\n",
    "\n",
    "# nerel_bio_labels_test['DISEASE'] = nerel_bio_labels_test['DISO']\n",
    "# del nerel_bio_labels_test['DISO']\n",
    "\n",
    "# test_counts_nerel = filter_keys(nerel_labels_test, common_labels)\n",
    "# test_counts_nerel_bio = filter_keys(nerel_bio_labels_test, common_labels)\n",
    "\n",
    "# print(dict(sorted(test_counts_nerel.items())))\n",
    "# print(dict(sorted(test_counts_nerel_bio.items())))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqiXYBpSz1qW",
    "outputId": "5c093af7-56f0-4c18-8d79-8728a7b76e3c"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# path_text = '/content/drive/MyDrive/NEREL/NEREL-v1.1/test/113534_text.txt'\n",
    "# path_ann = '/content/drive/MyDrive/NEREL/NEREL-v1.1/test/113534_text.ann'\n",
    "\n",
    "# path_text = '/content/drive/MyDrive/NEREL/NEREL-v1.1/test/195702_text.txt'\n",
    "# path_ann = '/content/drive/MyDrive/NEREL/NEREL-v1.1/test/195702_text.ann'\n",
    "\n",
    "# path_text = '/content/drive/MyDrive/NEREL/NEREL-v1.1/train/133570_text.txt'\n",
    "# path_ann = '/content/drive/MyDrive/NEREL/NEREL-v1.1/train/133570_text.ann'\n",
    "\n",
    "path_text = '/content/drive/MyDrive/NEREL/nerel-bio-v1.0/test/22271960_ru.txt'\n",
    "path_ann = '/content/drive/MyDrive/NEREL/nerel-bio-v1.0/test/22271960_ru.ann'\n",
    "\n",
    "\n",
    "json = file_to_json(path_text)\n",
    "tokens = json['tokens']\n",
    "# toks_nums = []\n",
    "# for i in range(len(tokens)):\n",
    "#   toks_nums.append(tokens[i] + ';' + str(i))\n",
    "# print(toks_nums)\n",
    "text = get_text(path_text)\n",
    "anns = get_sentence_anns(path_text)\n",
    "\n",
    "for i in range(len(anns)):\n",
    "  print([word + ';' + str(j) for j, word in enumerate(anns[i]['tokens'])])\n",
    "  print(anns[i]['entity_mentions'])\n",
    "  print('-----------------------------------------------------------------------------------------')\n",
    "# valid_labeling(path_text, debug=True)"
   ],
   "metadata": {
    "id": "IwHXWo-CZaJ2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7c27c4f3-9211-4166-a0ef-2ae9233b0384"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "3aWCrkQ5gA1W"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
